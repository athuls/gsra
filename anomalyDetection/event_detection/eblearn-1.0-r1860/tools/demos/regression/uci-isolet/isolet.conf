################################################################################
# uci-isolet regression configuration file

name = isolet
root = /data/regression/uci-isolet/

# energies & answers ###########################################################
trainer = trainable_module1
answer = regression_answer1
trainable_module1_energy = l2_energy
regression_answer1_threshold = .05 # answers closer than this are correct

# architecture #################################################################
arch = ${f1},${f2}
nonlin = tanh # stdsig # non-linearity module

f1 = linear1,addc1,${nonlin}
f2 = linear2,addc2 #,${nonlin}

linear1_in = 617
linear1_out = 80 #20 # 10 40 80 100
linear2_in = ${linear1_out}
linear2_out = noutputs

# training #####################################################################
dsname = isolet
train = ${root}/${dsname}-train.mat # training set
train_labels = ${root}/${dsname}-train-labels.mat # training set
val = ${root}/${dsname}-test.mat # validation set
val_labels = ${root}/${dsname}-test-labels.mat # validation set
data_bias = 0 # bias added to data (before coeff multiplication)
data_coeff = 1 # coeff multiplied to data (after bias addition)
label_bias = -10 # bias added to labels (before coeff multiplication)
label_coeff = .1 # coeff multiplied to labels (after bias addition)

eta = .001 # .0005 .0001 .00005 .00001 .000005 .000001 # learning rate
reg = 0 # .001 .0005 .0001 .00005 .00001
reg_l1 = ${reg} #.0001 # L1 regularization
reg_l2 = ${reg} #.0001 # L2 regularization
reg_time = 0 # time (in samples) after which to start regularizing
inertia = 0.0 # gradient inertia
anneal_value = 0.0 # learning rate decay value
annea_period = 0 # period (in samples) at which to decay learning rate
gradient_threshold = 0.0
iterations = 300 # number of training iterations
ndiaghessian = 100 # 200 400 800 1200 # #samples for 2nd derivatives estimation
epoch_mode = 1 # 0: fixed number 1: show all at least once
#epoch_size = 2000 # number of training samples per epoch. comment to ignore.
epoch_show_modulo = 3000 # print message every n training samples
sample_probabilities = 0 # use probabilities to pick samples
hardest_focus = 1 # 0: focus on easiest samples 1: focus on hardest ones
ignore_correct = 1 # If 1, do not train on correctly classified samples
min_sample_weight = 0 #.1 .5 1 # minimum probability of each sample
per_class_norm = 1 # normalize probabiliy by class (1) or globally (0)
shuffle_passes = 1 # shuffle samples between passes
balanced_training = 1 # show each class the same amount of samples or not
no_training_test = 0 # do not test on training set if 1
no_testing_test = 0 # do not test on testing set if 1
save_pickings = 0 # save sample picking statistics
test_only = 0 # if 1, just test the data and return
save_weights = 0 # if 0, do not save weights after each iteration

# training display #############################################################
show_conf = 0 # print configuration or not
show_train = 0 # enable/disable all training display
show_train_ninternals = 0 # number of internal examples to display
show_train_errors = 0 # show worst errors on training set
show_train_correct = 0 # show worst corrects on training set
show_val_errors = 1 # show worst errors on validation set
show_val_correct = 1 # show worst corrects on validation set
show_hsample = 5 # number of samples to show on height axis
show_wsample = 18 # number of samples to show on height axis
show_wait_user = 0 # if 1, wait for user to close windows, otherwise close them.

# retraining ###################################################################
retrain = 0
retrain_weights =

# meta training ################################################################
meta_command = train
meta_name = isolet # name of this meta job
meta_max_jobs = 12 # maximum number of jobs to run at the same time
#meta_output_dir = ${root}/../out/ # directory where to write outputs
meta_output_dir = ${HOME}/o/isolet/ # directory where to write outputs
meta_gnuplot_params="set grid ytics;set ytics;set mytics;set grid mytics;set logscale y; set mxtics; set grid xtics; set pointsize 0.5; set key spacing .5;" # extra gnuplot parameters
meta_gnuplot_terminal="postscript"
#meta_gnuplot_font="Times=6" # font options
#meta_gnuplot_line="lw .1" # line options
# analyze processes output or not. if 0, the meta_trainer will have no notion
# of iteration and will only send 1 report at the very end.
# if 1, meta_trainer will try to find iteration and other variable values
# in each process' output.
meta_analyze = 0
meta_send_email = 1 # emailing results or not
meta_email=${myemail} # email to use (use environment variable "myemail")
# iterations at which to send an email
meta_email_iters = 0,1,2,3,4,5,7,10,15,20,30,50,75,100,200
meta_email_period = 1 # send email with this freq (if email_iters not defined)
meta_watch_interval = 30 # interval sec to analyze outputs and check who's alive
# variables to minimize, process and iteration with lowest value will
# be used to report best weights, or start consequent training
meta_minimize = test_errors,errors,test_energy,energy
meta_ignore_iter0 = 1 # do not take results for i = 0 into account
meta_sticky_vars = meta_conf_shortname,job,config,classes,eta,reg # vars to keep around at each iterations
meta_watch_vars = #job,1FPPI,.01FPPI # restrict variable watching to those
meta_nbest = 5 # number of best answers to show/send
meta_send_best = 0 # if 1 send best answers files minimizing meta_minimize's value
meta_send_logs = 0 # send logs of all jobs or not
meta_no_conf_id = 0 # do not use conf ids for naming
meta_best_keycomb = eta,reg,arch,classifier # print best of each possible comb
meta_plot_keys = ntype,tenergy,i # plot all combs vs last one as x-axis
meta_timeout = 1200 # scheduler considers a job is not running after n seconds
meta_display_all = 0 # display all jobs results while metaparsing
meta_hierarchy = meta_conf_shortname,i # the hierarchy of parsed elements
meta_job_var = meta_conf_shortname # defines the job variable name

# classification ###############################################################
# uncomment lines below to train in classification mode

# classification = 1
# answer = class_answer
# label_bias = 0 # bias added to labels (before coeff multiplication)
# label_coeff = 1 # coeff multiplied to labels (after bias addition)

