################################################################################
# metarun configuration
# Note: variables starting with "meta_" are reserved for meta configuration

meta_command = train # command to run
# optional meta variables ######################################################
meta_name = ${name}_${machine} # name of this meta job
meta_max_jobs = 12 # maximum number of jobs to run at the same time
meta_output_dir = ${root}/../out/ # directory where to write outputs
meta_gnuplot_params="set grid ytics;set ytics;set mytics;set grid mytics;set logscale y; set mxtics; set grid xtics; set pointsize 0.5; set key spacing .5;" # extra gnuplot parameters
meta_gnuplot_terminal="pdf"
meta_gnuplot_font="Times=6" # font options
meta_gnuplot_line="lw .1" # line options
# analyze processes output or not. if 0, the meta_trainer will have no notion
# of iteration and will only send 1 report at the very end.
# if 1, meta_trainer will try to find iteration and other variable values
# in each process' output.
meta_analyze = 1
meta_send_email = 1 # emailing results or not
meta_email=${myemail} # email to use (use environment variable "myemail")
# iterations at which to send an email
meta_email_iters = 0,1,2,3,4,5,7,10,15,20,30,50,75,100,200
meta_email_period = 1 # send email with this freq (if email_iters not defined)
meta_watch_interval = 30 # interval sec to analyze outputs and check who's alive
# variables to minimize, process and iteration with lowest value will
# be used to report best weights, or start consequent training
meta_minimize = AUC0-1,test_errors,errors,test_energy,energy,1FPPI,.01FPPI
meta_ignore_iter0 = 1 # do not take results for i = 0 into account
meta_sticky_vars = job,config,classes # vars to keep around at each iterations
meta_watch_vars = #job,1FPPI,.01FPPI # restrict variable watching to those
meta_nbest = 5 # number of best answers to show/send
meta_send_best = 0 # if 1 send best answers files minimizing meta_minimize's value
meta_send_logs = 0 # send logs of all jobs or not
meta_no_conf_id = 0 # do not use conf ids for naming
meta_best_keycomb = arch,classifier # print best of each possible comb

################################################################################
# local program configuration

name=inria
machine = ${HOSTNAME}a
ebl= ${HOME}/eblpierre/ # eblearn root
root = ${HOME}/${machine}data/ped/${name}/ds/ # datasets root
#root2 = ${ebl}/demos/pedestrians/trained/ # trained weights root

# network high level switches ##################################################

run_type = detect #train # detect fprop
net_type = cscsc #cscscf
net = mr2c${color} #sr2c # sr1c mr1c # net archictecture
manual_load = 1 # manually load weights for individual modules
color = 1
norm = 0

# architecture #################################################################

arch = ${pp_${run_type}},${arch_${net}_${net_type}} # global architecture
mirror = 0 # mirror instead of zero-padding (default)
nonlin = tanh # stdsig # non-linearity module

# normalization
norm00_0 =
norm0_0 =
norm2_0 =
norm00_1 = wstd00
norm0_1 = wstd0
norm2_1 = wstd2

# machines
arch_mr1_cscsc = ${c0},${s1},${c2},${s3},branch5,merge21,${f5}
arch_mr2_cscsc = ${c0},${s1},branch2,${c2},${s3},branch5,merge22,${f5}
arch_sr1c_cscsc = branch1,${c0},${s1},merge1,${c2},${s3},${c5}
arch_mr1c_cscsc = branch1,${c0},${s1},merge1,${c2},${s3},branch5,merge21,${f5}
#arch_mr2c_cscsc = branch1,${c0},${s1},merge1,branch2,${c2},${s3},branch5,merge22,${f5}
#arch_mr2c_cscscf = branch1,${c0},${s1},merge1,branch2,${c2},${s3},branch5,merge22,${f6},${f7}
arch_sr2c_cscsc = branch1,${c0},${s1},merge1,${c2},${s3},${c5}
arch_mr2c0_cscsc = ${c0},${s1},branch2,${c2},${s3},merge22,${f5}
arch_mr2c1_cscsc = branch1,${c0},${s1},merge1,branch2,${c2},${s3},merge22,${f5}
arch_mr2c1_cscscf = branch1,${c0},${s1},merge1,branch2,${c2},${s3},merge22,${f6},${f7}

branch1 = resize00,${c00} # color branch
branch2 = ${s20} # 1st multi-scale branch (biggest)
#branch2 = ${s20},branch3 # 1st multi-scale branch (biggest)
branch3 = ${s21},${branch4} # 1st multi-scale branch (medium)
branch4 = ${s22} # 1st multi-scale branch (smallest)
branch5 = ${s50},branch6 # 2nd multi-scale branch (medium)
branch6 = ${s51} # 2nd multi-scale branch (smallest)

# main branch layers
c0 = conv0,addc0,${nonlin},diag0,abs0,${norm0_${norm}}
s1 = subs1,addc1,${nonlin}
c2 = conv2,addc2,${nonlin},diag2,abs2,${norm2_${norm}}
s3 = subs3,addc3,${nonlin}
c5 = conv5,addc5,${nonlin}
f5 = linear5,addc5,${nonlin}
f6 = linear6,addc6,${nonlin}
f7 = linear7,addc7,${nonlin}

# color branch layers
c00 = conv00,addc00,${nonlin}00,diag00,abs00,${norm00_${norm}}

# multi scale branches layers
s20 = subs00,addc,${nonlin}
s21 = ${s20}
s22 = ${s20}
s50 = ${s20}
s51 = ${s20}

# branches parameters
branch1_type = narrow # feed UV color into branch
branch1_narrow_dim = 0
branch1_narrow_size = 2 # U and V
branch1_narrow_offset = 1 
branch2_type = copy # feed all features to multi-scale branch
branch3_type = copy # keep 1st layer scale 1 data
branch4_type = copy # keep 1st layer scale 2 data
branch5_type = copy # keep 2nd layer scale 2 data
branch6_type = copy # keep 2nd layer scale 3 data
merge1_type = concat
merge1_branches = branch1
merge1_concat_dim = 0
merge21_type = flat
merge21_branches = branch5,branch6
merge22_type = flat
#merge22_branches = branch2,branch3,branch4,branch5,branch6

merge22_branches = branch2
merge22_in = 16x8
merge22_branches_in = 20x12
merge22_stride = 1x1
merge22_branches_stride = 1x1

# merge22_inh = 16
# merge22_inw = 8
# merge22_branches_inh = 20,10,5,8,4
# merge22_branches_inw = 12,6,3,4,2
# merge22_strideh = 4
# merge22_stridew = ${merge22_strideh}
# merge22_branches_strideh = 4,2,1,2,1
# merge22_branches_stridew = ${merge22_branches_strideh}

conv00_kernel = 5x5 # convolution kernel sizes (hxw)
conv00_stride = 1x1 # convolution stride sizes (hxw)
conv00_table = ${table00} # convolution table (optional)
conv00_table_in = 2 # conv input max, used if table file not defined
conv00_table_out = 6 # features max, used if table file not defined
conv00_weights = ${wroot0c}${sp0}_layer1_convolution_kernel.mat
addc00_weights = ${wroot0c}${sp0}_layer2_bias_bias.mat
diag00_weights = ${wroot0c}${sp0}_layer4_diag_coeff.mat
wstd00_kernel = ${conv00_kernel} # normalization kernel sizes (hxw)
subs00_kernel = 2x2 # subsampling kernel sizes (hxw)
subs00_stride = ${subs00_kernel} # subsampling stride sizes (hxw)
resize00_hratio = 0.349206349 # 44 / 126
resize00_wratio = 0.358974359 # 28 / 78

# main branch parameters
inputh = 126 # input's height
inputw = 78 # input's width
conv0_kernel = 7x7 # convolution kernel sizes (hxw)
conv0_stride = 1x1 # convolution strides  (hxw)
conv0_table = #${table0_color${color}} # convolution table (optional)
conv0_table_in = 1 # conv input max, used if table file not defined
conv0_table_out = ${table0_max_color${color}} # features max, used if table file not defined
conv0_weights = ${wroot0}${sp0}_layer1_convolution_kernel.mat
addc0_weights = ${wroot0}${sp0}_layer2_bias_bias.mat
diag0_weights = ${wroot0}${sp0}_layer4_diag_coeff.mat
wstd0_kernel = 7x7 # normalization kernel sizes (hxw)
subs1_kernel = 3x3 # subsampling kernel sizes (hxw)
subs1_stride = ${subs1_kernel} # subsampling strides (hxw)
addc1_weights = # weights to be loaded if manual_load = 1
conv2_kernel = 9x9 # convolution kernel sizes (hxw)
conv2_stride = 1x1 # convolution strides (hxw)
conv2_table = ${table1} # convolution table (optional)
conv2_table_in = thickness # use current thickness as max table input
conv2_table_out = ${table1_max} # features max, used if table file not defined
conv2_weights = ${wroot1c}${sp0}${sp1}_layer1_convolution_kernel.mat
addc2_weights = ${wroot1c}${sp0}${sp1}_layer2_bias_bias.mat
diag2_weights = ${wroot1c}${sp0}${sp1}_layer4_diag_coeff.mat
wstd2_kernel = ${conv2_kernel} # normalization kernel sizes (hxw)
subs3_kernel = 2x2 # subsampling kernel sizes (hxw)
subs3_stride = ${subs3_kernel} # subsampling strides (hxw)
addc3_weights = # weights to be loaded if manual_load = 1
linear5_in = ${linear5_in_${net}} #thickness # linear module input features size
linear5_out = noutputs # use number of classes as max table output
linear6_in = ${linear6_in_${net}} #thickness # linear module input features size
linear6_out = 10 # use number of classes as max table output
linear7_in = ${linear6_out} #thickness # linear module input features size
linear7_out = noutputs # use number of classes as max table output
conv5_kernel = 16x8 # convolution kernel sizes (hxw)
conv5_stride = 1x1 # convolution strides (hxw)
conv5_table_in = thickness # use current thickness as max table input
conv5_table_out = noutputs # features max, used if table file not defined

linear5_in_mr1 = 10752
linear5_in_mr2c0 = 20832
linear5_in_mr1c = 11424
linear5_in_mr2c1 = 23394

linear5_in_mr2c0 = 17824
linear5_in_mr2c1 = 17824
linear6_in_mr2c1 = 17824 #8704

# manual loading ##############################################################

sp0 = 12 #0703 2007 # 0301 1205 #03 07 12 # sparsity layer0
sp1 = 12 #03 07 12 # sparsity layer1

#wroot0 = /home/pierre/eblpierre/tools/demos/pedestrians/inria/koray/pedmachines/machine5732
#wroot0 = ${HOME}/koray/color/machine57
#wroot0 = /data/koray/pedmachines/machine5732
#wroot1 = /data/koray/pedmachines2/machine95732

wroot0 = /data/koray/pedmachines_Y/machine50732
wroot0c = /data/koray/pedmachines_UV/machine50706
wroot1c = /data/koray/pedmachines_YUV/machine

# tables #######################################################################

tblroot = ${ebl}/tools/data/tables/ # location of table files

# conv00
table00 = ${tblroot}/table_2_6_connect_6_fanin_0_density_0.5_uv0_u3_v6.mat

# conv0
table0_max_color0 = 38
table0_max_color1 = 32 #32 #64 # full table output max (overridden if table file defined)
# table0_color0 = # no color, use table0_max for full table
# table0_color1 = ${tblroot}/table_3_32_connect_32_fanin_0_density_0.33_yuv0_y26_u
29_v32.mat

# conv1
tbl=3
table1_max = 68 # 64 96 128 256
tbl1_mr1 = ${tblroot}/table_32_64_connect_1664_fanin_26_density_0.81_random.mat
tbl1_mr2c0 = ${tbl1_mr2c1}
tbl1_2 = ${tblroot}/table_32_96_connect_2496_fanin_26_density_0.81_random.mat
tbl1_sr1c = ${tblroot}/table_38_68_connect_2040_fanin_30_density_0.79_random.mat
tbl1_mr1c = ${tblroot}/table_38_68_connect_2040_fanin_30_density_0.79_random.mat
tbl1_mr2c1 = ${tblroot}/table_38_68_connect_2040_fanin_30_density_0.79_random.mat
tbl1_sr2c = ${tbl1_mr2c}

#tbl_mrc = ${tbl1_3}
#tbl_mr = ${tbl1_1}

table1 = ${tbl1_${net}}

# preprocessing ################################################################
preprocessing = 1 # 0: none 1: contrast normalization (optional)
resize = mean # bilinear
normalization_size = 7 # 9

# preprocessing modules
pp_y = rgb_to_y0
pp_yuv = rgb_to_yuv0
pp_yp = rgb_to_yp0
pp_ypuv = rgb_to_ypuv0
rgb_to_ypuv0_kernel = 7x7
resizepp0_pp = rgb_to_ypuv0
#resizepp0_fovea = ${fovea}
resizepp0_zpad = 
pp_train = #rgb_to_ypuv0 #mschan0
pp_detect = resizepp0

# training #####################################################################
ds = 1 # dataset id
jitter = _jitter_4_2_8_.10_5
dsname = ${name}_${resize}${inputh}x${inputw}_ker${normalization_size}${jitter}_bg
train = ${dsname}_train_${ds} # training set
val = ${dsname}_val_${ds} # validation set

#root = /data/pedestrians/small/
#train = small
#val = small

#reg = 0 .000001
reg = 0 #.0001
#reg = .001 .0001

#eta = .000001 .00001  # learning rate
#eta = .0001 .00001 .000001 .0000001 # learning rate
#eta = .001 #.0001 .00001 .000001 # learning rate
#eta = .0000001 .0000005 .000001 .0000025 .000005 .0000075 #.00001 #.00002 # learning rate
eta = .0000005 .000005 .00001 # learning rate
reg_l1 = ${reg} #.0001 # L1 regularization
reg_l2 = ${reg} #.0001 # L2 regularization
reg_time = 0 # time (in samples) after which to start regularizing
inertia = 0.0 # gradient inertia
anneal_value = 0.0 # learning rate decay value
annea_period = 0 # period (in samples) at which to decay learning rate
gradient_threshold = 0.0
iterations = 20 # number of training iterations
ndiaghessian = 400 #800 1200 # number of sample for 2nd derivatives estimation
epoch_mode = 0 # 0: fixed number 1: show all at least once
epoch_size = 4000 # number of training samples per epoch. comment to ignore.
epoch_show_modulo = 100 # print message every n training samples
sample_probabilities = 0 # use probabilities to pick samples
hardest_focus = 1 # 0: focus on easiest samples 1: focus on hardest ones
ignore_correct = 1 # If 1, do not train on correctly classified samples
min_sample_weight = 0 #.1 .5 1 # minimum probability of each sample
per_class_norm = 1 # normalize probabiliy by class (1) or globally (0)
shuffle_passes = 1 # shuffle samples between passes
balanced_training = 1 # show each class the same amount of samples or not
no_training_test = 1 # do not test on training set if 1
no_testing_test = 0 # do not test on testing set if 1
target_factor = 1 # multiply targets -1 and 1 by:
save_pickings = 1 # save sample picking statistics
binary_target = 0 # use only 1 output, -1 for negative examples, +1 for positive
test_only = 0 # if 1, just test the data and return

# training display #############################################################
show_train = 1 # enable/disable all training display
show_train_ninternals = 0 # number of internal examples to display
show_train_errors = 0 # show worst errors on training set
show_val_errors = 1 # show worst errors on validation set
show_val_correct = 1 # show worst corrects on validation set
show_hsample = 5 # number of samples to show on height axis
show_wsample = 18 # number of samples to show on height axis
show_wait_user = 0 # if 1, wait for user to close windows, otherwise close them.

# retraining ###################################################################
retrain = 0
retrain_weights = # ${root1}/${job_name_retraining}_net040.mat

# detection ####################################################################
net_min_height = ${inputh}
net_min_width = ${inputw}
nthreads = 1 # number of detection threads
threads_loading = 0 # threads load the data themselves
ipp_cores = 1 # number of cores used by IPP
weights = ${root2}${weights_file}
#classes = ${root2}${job_name}_classes.mat
classes = ${root2}${train}_classes.mat
threshold = .1 # confidence detection threshold
gain = 1
input_height = -1 # use -1 to use original size
input_width = -1 # use -1 to use original size
input_min = 0 # minimum height or width for minimum scale
input_max = 1200 # maximum height or width for maximum scale
# multi-scaling type. 0: manually set each scale sizes, 1: manually set each
# scale step, 2: number of scales between min and max, 3: step factor between
# min and max, 4: 1 scale, the original image size.
scaling_type = 3
scaling = 1.09 # scaling ratio between scales
min_scale = 1 #.75 # min scale as factor of minimal network size
max_scale = 1.3 # max scale as factor of original resolution
input_random = 1 # randomize input list (only works for 'directory' camera).
input_npasses = 1 # passes on the input list (only works for 'directory' cam).
hzpad = .3 # vertical zero padding on each side as ratio of network's min input
wzpad = .3 # horizontal zero padding on each side as ratio of network's min in
#mem_optimization = 1
bbox_saving = 2 # 0: none 1: all styles 2: eblearn style 3: caltech style
max_object_hratio = 0 #13.5 # image's height / object's height, 0 to ignore
smoothing = 0 # smooth network outputs
background_name = bg # name of background class (optional)

# nms (non-maximum suppression) ################################################
nms = 1 # 0: no pruning 1: ignore overlapping bb 2: pedestrian custom
bbox_file = #bbox.txt # ignore processing, feed pre-computed bboxes to nms
bbhfactor = .75 # height factor to apply to bounding boxes
bbwfactor = .5 # width factor to apply to bounding boxes
confidence_type = 2 # 0: sqrdist 1: single output 2: max other (recommended)
max_bb_overlap = .6 # minimum ratio with smallest bbox to declare overlap
min_hcenter_dist = .3 # centers closer than this ratio over height cancel out
min_wcenter_dist = .1 # centers closer than this ratio over width cancel out

# detection display ############################################################
skip_frames = 0 # skip this number of frames before each processed frame
save_detections = 0 # output saving and display
save_max = 25000 # Exit when this number of objects have been saved
save_max_per_frame = 10 # Only save the first n objects per frame
save_video = 0 # save each classified frame and make a video out of it
save_video_fps = 5
use_original_fps =0
display = 1
display_zoom = 1 # zooming
display_min = -1.7 # minimum data range to display (optional)
display_max = 1.7 # maximum data range to display (optional)
display_in_min = 0 # input image min display range (optional)
display_in_max = 255 # input image max display range (optional)
display_bb_transparency = .5 # bbox transp factor (modulated by confidence)
display_threads = 0 # each thread displays on its own
display_states = 0 # display internal states of 1 resolution
show_parts = 0 # show parts composing an object or not
silent = 0 # minimize outputs to be printed
sync_outputs = 1 # synchronize output between threads
minimal_display = 1 # only show classified input
display_sleep = 0 # sleep in milliseconds after displaying
ninternals = 1
# demo display variables
queue1 = 0
qstep1 = 1
qheight1 = 5
qwidth1 = 2
queue2 = 0
qstep2 = 50
qheight2 = 5
qwidth2 = 5
precamera = 0 # pre-camera (used before regular camera)
precamdir = ${root2}/

camera = directory # camera options: opencv shmem video directory
# specify a custom image search pattern (optional)
file_pattern = #".*[.](png|jpg|jpeg|PNG|JPG|JPEG|bmp|BMP|ppm|PPM|pnm|PNM|pgm|PGM|gif|GIF)"
# limit of input video duration in seconds, 0 means no limit
input_video_max_duration = 0 
# step between input frames in seconds, 0 means no step
input_video_sstep = 0

# evaluation ###################################################################
set = Train # Test
evaluate = 0
evaluate_cmd = "${visiongrader} ${visiongrader_params}"
visiongrader = ${HOME}/visiongrader/src/main.py
visiongrader_params = "${input_params} ${groundtruth_params} ${compare_params} ${curve_params} ${ignore} "
input_params = "--input bbox.txt --input_parser eblearn --sampling 50 "
#annotations = ${root}/../INRIAPerson/${set}/annotations/
annotations = /data/sermanet/ped/inria/small/${set}/annotations/
groundtruth_params = "--groundtruth ${annotations} --groundtruth_parser inria --gt_whratio .43 "
compare_params = "--comparator overlap50percent --comparator_param .5 "
curve_params = "--det --saving-file curve.pickle --show-no-curve "
#input_dir = /home/sermanet/${machine}data/ped/inria/INRIAPerson/${set}/pos
input_dir = /data/sermanet/ped/inria/small/${set}/pos/
input_list = crop001102.png,crop001614.png,crop001536.png,crop001616.png
ignore = "--ignore ${HOME}/visiongrader/datasets/pedestrians/inria/ignore/${set}/ "

detection_test = 1 # test detection during training
detection_test_nthreads = 1
detection_params = run_type=detect\nscaling=1.2\nthreshold=.1\nroot2=./\nclasses=${root}/${train}_classes.mat

# training-generated variables #################################################

# root2 = ${HOME}/projects/ped/inria/trained/20101228.064539.inria_rose3a/
# job_name = 20101228.064539.inria_rose3a_09_retraining_conf00_eta_.000005_reg_0
# weights_file = ${job_name}_net017.mat
# #retrain = 1
# retrain_weights = ${root2}${weights_file}
# #retrain_iteration = 18
# classes = ${root2}/20101228.064539.inria_rose3a_09_retraining_conf00_eta_.000005_reg_0_classes.mat

# max_scale = .6
# input_dir = /data/pedestrians/inria/INRIAPerson/Test/pos
# meta_output_dir = ${HOME}/tmp/
# next_on_key = n
# input_max = 1100
# input_random = 0
# run_type = detect
# minimal_display = 0
# display_threads = 1

retrain = 1
retrain_weights = ${root2}${weights_file}
root2 = /home/sermanet/projects/ped/inria/trained/20110518.115314.inria_iagoa/
classes = ${root2}/2_inria_mean126x78_ker7_jitter_4_2_8_.10_5_bg_train_1_classes.mat
#weights_file = 20110518.115314.inria_iagoa_03_retraining_conf4_eta_.000005_norm_1_net00004.mat
weights_file = 20110518.115314.inria_iagoa_00_training_conf2_eta_.00001_norm_0_net00020.mat
